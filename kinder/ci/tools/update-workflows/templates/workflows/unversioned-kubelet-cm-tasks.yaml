# IMPORTANT! this workflow is imported by unversioned-kubelet-cm-* workflows.
version: 1
summary: |
  This workflow implements a sequence of tasks used test the proper functioning
  of kubeadm with the control-plane running as non-root.
vars:
  # vars defines default values for variable used by tasks in this workflow;
  # those values might be overridden when importing this files.
  kubernetesVersion: v1.13.5
  upgradeVersion: v1.13.5
  controlPlaneNodes: 3
  workerNodes: 2
  baseImage: kindest/base:v20191105-ee880e9b # has containerd
  image: kindest/node:test
  clusterName: kinder-unversioned-kubelet-cm
  kubeadmVerbosity: 6
tasks:
- name: pull-base-image
  description: |
    pulls kindest/base image with docker in docker and all the prerequisites necessary for running kind(er)
  cmd: docker
  args:
  - pull
  - "{{ .vars.baseImage }}"
- name: add-kubernetes-versions
  description: |
    creates a node-image-variant by adding a Kubernetes version
  cmd: kinder
  args:
  - build
  - node-image-variant
  - --base-image={{ .vars.baseImage }}
  - --image={{ .vars.image }}
  - --with-init-artifacts={{ .vars.kubernetesVersion }}
  - --with-upgrade-artifacts={{ .vars.upgradeVersion }}
  - --loglevel=debug
  timeout: 15m
- name: create-cluster
  description: |
    create a set of nodes ready for hosting the Kubernetes cluster
  cmd: kinder
  args:
  - create
  - cluster
  - --name={{ .vars.clusterName }}
  - --image={{ .vars.image }}
  - --control-plane-nodes={{ .vars.controlPlaneNodes }}
  - --worker-nodes={{ .vars.workerNodes }}
  - --loglevel=debug
  timeout: 5m
- name: init
  description: |
    Initializes the Kubernetes cluster with version "initVersion"
    by starting the boostrap control-plane nodes
  cmd: kinder
  args:
  - do
  - kubeadm-init
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  - --kubeadm-verbosity={{ .vars.kubeadmVerbosity }}
  - --kubeadm-feature-gate="UnversionedKubeletConfigMap=true"
  - --copy-certs=auto
  timeout: 5m
- name: verify-resources-exist-pre-upgrade
  description: |
    Verify that the ConfigMap and RBAC objects with the new name exist (UnversionedKubeletConfigMap=true)
  cmd: /bin/bash
  args:
    - -c
    - |
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get role kubeadm:kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get rolebinding kubeadm:kubelet-config -n kube-system || exit 1
  timeout: 5m
- name: join
  description: |
    Join the other nodes to the Kubernetes cluster
  cmd: kinder
  args:
  - do
  - kubeadm-join
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  - --kubeadm-verbosity={{ .vars.kubeadmVerbosity }}
  - --copy-certs=auto
  timeout: 10m
- name: undo-feature-gate-changes
  description: |
    Restore the old ConfigMap and RBAC names, pre-upgrade.
    With the feature gate set, upgrade should create the objects with the new naming format
  cmd: /bin/bash
  args:
    - -c
    - |
      # get the major.minor version
      VERSION=$(echo {{ .vars.kubernetesVersion }} | cut -d 'v' -f 2 | cut -d '.' -f1,2)
      echo VERSION=$VERSION

      # duplicate the new naming format objects back to the old naming format objects
      docker exec {{ .vars.clusterName }}-control-plane-1 bash -c "kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubelet-config -n kube-system -o=yaml | sed s/'  name: kubelet-config/  name: kubelet-config-$VERSION'/ | kubectl --kubeconfig /etc/kubernetes/admin.conf create -f - " || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 bash -c "kubectl --kubeconfig /etc/kubernetes/admin.conf get role kubeadm:kubelet-config -n kube-system -o=yaml | sed s/'  name: kubeadm:kubelet-config/  name: kubeadm:kubelet-config-$VERSION'/ | sed s/'  - kubelet-config/  - kubelet-config-$VERSION'/ | kubectl --kubeconfig /etc/kubernetes/admin.conf create -f - " || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 bash -c "kubectl --kubeconfig /etc/kubernetes/admin.conf get rolebinding kubeadm:kubelet-config -n kube-system -o=yaml | sed s/'  name: kubeadm:kubelet-config/  name: kubeadm:kubelet-config-$VERSION'/ | kubectl --kubeconfig /etc/kubernetes/admin.conf create -f - " || exit 1

      # delete the new format objects
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf delete cm kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf delete role kubeadm:kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf delete rolebinding kubeadm:kubelet-config -n kube-system || exit 1
  timeout: 5m
- name: upgrade
  description: |
    Upgrade the cluster to Kubernetes "upgradeVersion"
  cmd: kinder
  args:
  - do
  - kubeadm-upgrade
  - --upgrade-version={{ .vars.kubernetesVersion }}
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  - --kubeadm-verbosity={{ .vars.kubeadmVerbosity }}
  timeout: 15m
- name: verify-resources-exist-post-upgrade
  description: |
    Verify that the ConfigMap and RBAC objects with the new name exist (UnversionedKubeletConfigMap=true)
  cmd: /bin/bash
  args:
    - -c
    - |
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get cm kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get role kubeadm:kubelet-config -n kube-system || exit 1
      docker exec {{ .vars.clusterName }}-control-plane-1 kubectl --kubeconfig /etc/kubernetes/admin.conf get rolebinding kubeadm:kubelet-config -n kube-system || exit 1
- name: cluster-info
  description: |
    Runs cluster-info
  cmd: kinder
  args:
  - do
  - cluster-info
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
- name: e2e-kubeadm
  description: |
    Runs kubeadm e2e tests
  cmd: kinder
  args:
  - test
  - e2e-kubeadm
  - --test-flags=--report-dir={{ .env.ARTIFACTS }} --report-prefix=e2e-kubeadm
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  timeout: 10m
- name: get-logs
  description: |
    Collects all the test logs
  cmd: kinder
  args:
  - export
  - logs
  - --loglevel=debug
  - --name={{ .vars.clusterName }}
  - "{{ .env.ARTIFACTS }}"
  force: true
  timeout: 5m
  # kind export log is know to be flaky, so we are temporary ignoring errors in order
  # to make the test pass in case everything else passed
  # see https://github.com/kubernetes-sigs/kind/issues/456
  ignoreError: true
- name: reset
  description: |
    Exec kubeadm reset
  cmd: kinder
  args:
  - do
  - kubeadm-reset
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  - --kubeadm-verbosity={{ .vars.kubeadmVerbosity }}
  force: true
- name: delete
  description: |
    Deletes the cluster
  cmd: kinder
  args:
  - delete
  - cluster
  - --name={{ .vars.clusterName }}
  - --loglevel=debug
  force: true
